In [[probability theory]], '''Bennett's [[inequality (mathematics)|inequality]]''' provides an [[upper bound]] on the [[probability]] that the sum of [[independent random variables]] deviates from its [[expected value]] by more than any specified amount. Bennett's inequality was proved by George Bennett of the [[University of New South Wales]] in 1962.<ref name=bennett>{{Cite journal | last1 = Bennett | first1 = G. | title = Probability Inequalities for the Sum of Independent Random Variables | journal = [[Journal of the American Statistical Association]] | volume = 57 | issue = 297 | pages = 33–45 | doi = 10.2307/2282438 | jstor = 2282438| year = 1962 | pmid =  | pmc = }}</ref>

== Statement ==
Let 
{{math|''X''<sub>1</sub>, … ''X''<sub>''n''</sub>}}
be [[independent random variables]] with finite variance and assume (for simplicity but [[without loss of generality]]) they all have zero expected value. Further assume {{math|''X''<sub>''i''</sub> ≤ ''a''}} [[almost surely]] for all {{math|''i''}}, and define <math> S_n = \sum_{i = 1}^n X_i</math> and <math> \sigma^2 = \sum_{i=1}^n \operatorname{E}(X_i^2).</math> 
Then for any {{math|''t'' ≥ 0}},

:<math>\Pr\left( S_n > t \right) \leq
\exp\left( - \frac{\sigma^2}{a^2} h\left(\frac{at}{\sigma^2} \right)\right),</math>

where {{math|''h''(''u'') {{=}} (1 + ''u'')log(1 + ''u'') – ''u''}}.<ref name=devroye>{{cite book|title=Combinatorial methods in density estimation| first1=Luc |last1=Devroye| authorlink1=Luc Devroye| first2=Gábor |last2=Lugosi| publisher=[[Springer (publisher)|Springer]]| year=2001| isbn=978-0-387-95117-1| page=11| url=https://books.google.com/books?id=jvT-sUt1HZYC&pg=PA11}}</ref><ref name=BLM2013>{{cite book|title=Concentration inequalities, a nonasymptotic theory of independence | first1=Stephane | last1=Boucheron | first2=Gabor|last2=Lugosi|first3=Pascal|last3=Massart|publisher=Oxford University Press|year=2013|ISBN=978-0-19-953525-5}}</ref>

== Generalizations and comparisons to other bounds ==
For generalizations see Freedman (1975)<ref>{{cite journal |title=On tail probabilities for martingales.| first1=D. A. |last1=Freedman| authorlink1=David A. Freedman |  publisher=[[The Annals of Probability]]|volume= 3| year=1975| pages=100–118|jstor=2959268 }}</ref> and Fan, Grama and Liu (2012)<ref>{{cite journal |title=Hoeffding's inequality for supermartingales| first1=X. |last1=Fan|  first2=I. |last2=Grama |first3=Q. |last3=Liu| journal=[[Stochastic Processes and their Applications]]|volume= 122| year=2012| pages=3545–3559 | doi=10.1016/j.spa.2012.06.009|arxiv=1109.4359}}</ref> for a [[Martingale (probability theory)|martingale]] version of Bennett's inequality and its improvement, respectively.

[[Hoeffding's inequality]] only assumes the summands are bounded almost surely, while Bennett's inequality offers some improvement when the variances of the summands are small compared to their almost sure bounds. However Hoeffding's inequality entails sub-Gaussian tails, whereas in general Bennett's inequality has Poissonian tails.{{cn|date=September 2016}}
In both inequalities, unlike some other inequalities or limit theorems, there is no requirement that the component variables have identical or similar distributions.{{cn|date=September 2016}}

==See also==
*[[Concentration inequality]] - a summary of tail-bounds on random variables.

==References==
<references />

[[Category:Probabilistic inequalities]]


{{probability-stub}}